import os
import shutil
import subprocess
import sys
import time
from pathlib import Path

try:
    import requests
except ImportError:
    requests = None


def print_header():
    line = "=" * 70
    print(line)
    print("TR·ª¢ L√ù AI - C·∫§U H√åNH OLLAMA CHATBOT".center(70))
    print(line)
    print()


def try_extend_path_with_common_ollama_locations() -> None:
    """
    Th√™m m·ªôt s·ªë ƒë∆∞·ªùng d·∫´n c√†i ƒë·∫∑t Ollama ph·ªï bi·∫øn v√†o PATH (ch·ªâ trong process n√†y)
    ƒë·ªÉ th·ª≠ t√¨m l·ªánh ollama n·∫øu PATH ch∆∞a c·∫≠p nh·∫≠t.
    """
    possible_dirs = [
        Path(os.getenv("LOCALAPPDATA", "")) / "Programs" / "Ollama",
        Path("C:/Program Files/Ollama"),
        Path("C:/Program Files (x86)/Ollama"),
    ]
    for d in possible_dirs:
        if d.exists():
            os.environ["PATH"] = f"{d};{os.environ['PATH']}"


def check_ollama_installed() -> bool:
    """Ki·ªÉm tra xem l·ªánh 'ollama' c√≥ s·∫µn trong PATH hay kh√¥ng (sau khi ƒë√£ th·ª≠ n·ªëi PATH)."""
    try_extend_path_with_common_ollama_locations()
    return shutil.which("ollama") is not None


def run_command(cmd, cwd=None) -> int:
    """Ch·∫°y l·ªánh h·ªá th·ªëng ƒë∆°n gi·∫£n, in log ra m√†n h√¨nh."""
    print(f"\n> ƒêang ch·∫°y l·ªánh: {' '.join(cmd)}")
    try:
        result = subprocess.run(cmd, cwd=cwd)
        print(f"> Ho√†n th√†nh (m√£ tho√°t = {result.returncode})")
        return result.returncode
    except FileNotFoundError:
        print("> L·ªñI: Kh√¥ng t√¨m th·∫•y l·ªánh:", cmd[0])
        return 1
    except Exception as e:
        print("> L·ªñI:", e)
        return 1


def ensure_deepseek_model():
    """
    G·ªçi 'ollama pull qwen2.5:7b' ƒë·ªÉ ch·∫Øc ch·∫Øn model c√≥ s·∫µn.
    N·∫øu model ƒë√£ t·ªìn t·∫°i th√¨ l·ªánh n√†y ch·ªâ ki·ªÉm tra r·∫•t nhanh.
    """
    print("\n--- B∆Ø·ªöC 2: K√©o (pull) model qwen2.5:7b cho Ollama ---")
    cmd = ["ollama", "pull", "qwen2.5:7b"]
    code = run_command(cmd)
    if code != 0:
        print(
            "\n‚ùå Kh√¥ng th·ªÉ pull model 'qwen2.5:7b'. "
            "Vui l√≤ng ki·ªÉm tra l·∫°i Ollama ho·∫∑c k·∫øt n·ªëi m·∫°ng."
        )
        sys.exit(1)
    print("‚úÖ Model qwen2.5:7b ƒë√£ s·∫µn s√†ng cho chatbot.")


def run_app_with_ollama():
    """
    Thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng cho chatbot d√πng Ollama + deepseek-r1,
    sau ƒë√≥ ch·∫°y app.py b·∫±ng c√πng Python interpreter hi·ªán t·∫°i.
    """
    print("\n--- B∆Ø·ªöC 3: Ch·∫°y app.py v·ªõi c·∫•u h√¨nh OLLAMA ---")

    # Thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng cho process con
    env = os.environ.copy()
    env["CHATBOT_PROVIDER"] = "ollama"
    # Co the thay doi model o day
    env["OLLAMA_MODEL"] = "qwen2.5:7b"

    cmd = [sys.executable, "app.py"]
    print(
        "\n> ƒêang kh·ªüi ƒë·ªông ·ª©ng d·ª•ng Flask (app.py) v·ªõi c·∫•u h√¨nh:\n"
        f"  - CHATBOT_PROVIDER = {env['CHATBOT_PROVIDER']}\n"
        f"  - OLLAMA_MODEL     = {env['OLLAMA_MODEL']}\n"
    )
    print("üí° Sau khi app ch·∫°y, h√£y m·ªü tr√¨nh duy·ªát v√†o Dashboard v√† d√πng chatbot AI.")
    print()
    # Ch·∫°y app.py, kh√¥ng b·∫Øt output ƒë·ªÉ b·∫°n th·∫•y log tr·ª±c ti·∫øp
    os.execve(sys.executable, cmd, env)


def is_ollama_responding(url: str = "http://localhost:11434/api/tags", timeout: int = 3) -> bool:
    if requests is None:
        return False
    try:
        resp = requests.get(url, timeout=timeout)
        return resp.status_code == 200
    except Exception:
        return False


def ensure_ollama_running():
    """
    Ki·ªÉm tra Ollama ƒë√£ ch·∫°y ch∆∞a; n·∫øu ch∆∞a th√¨ c·ªë g·∫Øng kh·ªüi ƒë·ªông 'ollama serve'
    trong n·ªÅn v√† ƒë·ª£i s·∫µn s√†ng (t·ªëi ƒëa 20 gi√¢y).
    """
    print("\n--- B∆Ø·ªöC 1B: ƒê·∫£m b·∫£o d·ªãch v·ª• Ollama ƒëang ch·∫°y ---")
    if is_ollama_responding():
        print("‚úÖ Ollama ƒë√£ s·∫µn s√†ng (HTTP 11434).")
        return

    print("‚ö†Ô∏è  Ch∆∞a th·∫•y Ollama ch·∫°y. ƒêang c·ªë g·∫Øng kh·ªüi ƒë·ªông 'ollama serve' trong n·ªÅn...")
    creation_flags = 0
    if os.name == "nt":
        try:
            creation_flags = subprocess.CREATE_NEW_PROCESS_GROUP | subprocess.DETACHED_PROCESS
        except Exception:
            creation_flags = 0

    try:
        subprocess.Popen(
            ["ollama", "serve"],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
            creationflags=creation_flags,
        )
    except Exception as e:
        print(f"‚ùå Kh√¥ng th·ªÉ kh·ªüi ƒë·ªông 'ollama serve': {e}")
        print("Vui l√≤ng m·ªü c·ª≠a s·ªï PowerShell kh√°c v√† ch·∫°y: ollama serve")
        sys.exit(1)

    # Ch·ªù d·ªãch v·ª• l√™n
    for i in range(20):
        time.sleep(1)
        if is_ollama_responding():
            print(f"‚úÖ Ollama ƒë√£ s·∫µn s√†ng sau {i+1} gi√¢y.")
            return
        else:
            print(f"... ch·ªù Ollama s·∫µn s√†ng ({i+1}s)")

    print("‚ùå Ollama v·∫´n ch∆∞a s·∫µn s√†ng. H√£y t·ª± ch·∫°y 'ollama serve' r·ªìi th·ª≠ l·∫°i.")
    sys.exit(1)


def main():
    print_header()

    print("--- B∆Ø·ªöC 1: Ki·ªÉm tra Ollama ƒë√£ c√†i ƒë·∫∑t ch∆∞a ---")
    if not check_ollama_installed():
        print("\n‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y l·ªánh 'ollama' trong PATH. ƒêang th·ª≠ c√†i t·ª± ƒë·ªông qua winget...")
        # Th·ª≠ c√†i qua winget (c·∫ßn Windows 10/11 v√† winget s·∫µn trong m√°y)
        code = run_command(["winget", "install", "--id", "Ollama.Ollama", "-e", "--silent"])
        if code != 0:
            print(
                "\n‚ùå Kh√¥ng th·ªÉ c√†i Ollama t·ª± ƒë·ªông qua winget.\n"
                "Vui l√≤ng t·ª± c√†i th·ªß c√¥ng:\n"
                "  1) T·∫£i v√† c√†i: https://ollama.com/download\n"
                "  2) ƒê√≥ng v√† m·ªü l·∫°i PowerShell / CMD\n"
                "  3) Ch·∫°y l·∫°i: python run_ollama_chatbot.py\n"
            )
            sys.exit(1)

        # Sau khi c√†i, th·ª≠ l·∫°i t√¨m l·ªánh
        if not check_ollama_installed():
            print(
                "\n‚ùå ƒê√£ th·ª≠ c√†i nh∆∞ng v·∫´n ch∆∞a t√¨m th·∫•y l·ªánh 'ollama'.\n"
                "H√£y ki·ªÉm tra PATH ho·∫∑c m·ªü c·ª≠a s·ªï PowerShell m·ªõi r·ªìi ch·∫°y l·∫°i:\n"
                "   python run_ollama_chatbot.py\n"
            )
            sys.exit(1)

    print("‚úÖ ƒê√£ t√¨m th·∫•y l·ªánh 'ollama'.")
    ensure_ollama_running()

    ensure_deepseek_model()
    run_app_with_ollama()


if __name__ == "__main__":
    main()


